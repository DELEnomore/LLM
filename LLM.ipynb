{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "elvLzytTDk_E"
      },
      "id": "elvLzytTDk_E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "from abc import abstractmethod\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n",
        "import os"
      ],
      "metadata": {
        "id": "WCunIz3FECZ6"
      },
      "id": "WCunIz3FECZ6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "dsrcse8nEE7z"
      },
      "id": "dsrcse8nEE7z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_CACHE_DIR = '.cache'\n",
        "MODEL_CACHE_DIR = _CACHE_DIR + '/model'\n",
        "DATASET_CACHE_DIR = _CACHE_DIR + '/dataset'\n",
        "OUTPUT_DIR = 'output'\n",
        "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "MODEL_OUTPUT_DIR = os.path.join(OUTPUT_DIR, MODEL_NAME + '-finetuned')"
      ],
      "metadata": {
        "id": "PB9KQnzBELfY"
      },
      "id": "PB9KQnzBELfY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetInterface:\n",
        "    PATH = ''\n",
        "\n",
        "    NAME = None\n",
        "\n",
        "    def __init__(self, tokenizer):\n",
        "        self.data = load_dataset(self.PATH, self.NAME, cache_dir=DATASET_CACHE_DIR)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def get_data(self):\n",
        "        return self.data\n",
        "\n",
        "    @abstractmethod\n",
        "    def tokenize_function(self, tokenize):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def build_prompt(input) -> str:\n",
        "        pass"
      ],
      "metadata": {
        "id": "oPSyMViHEMhd"
      },
      "id": "oPSyMViHEMhd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RuozhiDataset(DatasetInterface):\n",
        "\n",
        "    PATH = 'hfl/ruozhiba_gpt4'\n",
        "\n",
        "    def tokenize_function(self, example):\n",
        "        # 将instruction和output合并为一个输入\n",
        "        input = example['instruction']\n",
        "        target = example['output']\n",
        "        # 将 'instruction' 和 'output' 作为序列对进行编码\n",
        "        model_input = self.tokenizer(input, padding=\"max_length\", truncation=True, max_length=512)\n",
        "        # 对目标输出进行编码\n",
        "        labels = self.tokenizer(target, padding=\"max_length\", truncation=True, max_length=512)\n",
        "        labels[\"input_ids\"] = [[-100 if token == self.tokenizer.pad_token_id else token for token in token_list] for\n",
        "                               token_list in\n",
        "                               labels[\"input_ids\"]]\n",
        "        # 将标签放入模型输入字典中\n",
        "        model_input[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_input\n",
        "\n",
        "    def build_prompt(input) -> str:\n",
        "        pass"
      ],
      "metadata": {
        "id": "EeUKGH6GENtE"
      },
      "id": "EeUKGH6GENtE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(DatasetInterface):\n",
        "\n",
        "    PATH = 'FuYuwen117/test'\n",
        "\n",
        "    def tokenize_function(self, example):\n",
        "        # 将instruction和output合并为一个输入\n",
        "        input = example['input']\n",
        "        target = example['output']\n",
        "        # 将 'instruction' 和 'output' 作为序列对进行编码\n",
        "        model_input = self.tokenizer(input, padding=\"max_length\", truncation=True, max_length=10)\n",
        "        # 对目标输出进行编码\n",
        "        labels = self.tokenizer(target, padding=\"max_length\", truncation=True, max_length=10)\n",
        "        labels[\"input_ids\"] = [[-100 if token == self.tokenizer.pad_token_id else token for token in token_list] for\n",
        "                               token_list in\n",
        "                               labels[\"input_ids\"]]\n",
        "        # 将标签放入模型输入字典中\n",
        "        model_input[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_input\n",
        "\n",
        "    def build_prompt(input) -> str:\n",
        "        pass"
      ],
      "metadata": {
        "id": "x2biuph0EO4d"
      },
      "id": "x2biuph0EO4d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def fine_tune():\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=MODEL_NAME, cache_dir=MODEL_CACHE_DIR)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    dataset_instance = TestDataset(tokenizer)\n",
        "    data = dataset_instance.get_data()\n",
        "\n",
        "    tokenized_dataset = data.map(dataset_instance.tokenize_function, num_proc=4, batched=True)\n",
        "    splited_dataset = tokenized_dataset['train'].train_test_split(test_size=0.2)\n",
        "    train_dataset = splited_dataset['train']\n",
        "    val_dataset = splited_dataset['test']\n",
        "    # 检查并添加填充标记\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,  # 混合精度\n",
        "        device_map=\"auto\",  # 自动分配到 GPU\n",
        "        cache_dir=MODEL_CACHE_DIR\n",
        "    )\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,  # LoRA 的秩\n",
        "        lora_alpha=32,  # LoRA 的缩放因子\n",
        "        lora_dropout=0.05,  # Dropout 概率\n",
        "        bias=\"none\",  # LoRA bias 设置\n",
        "        task_type=\"CAUSAL_LM\",  # 任务类型：自回归文本生成\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "        # 如果需要根据具体模型结构，调整 target_modules\n",
        "    )\n",
        "\n",
        "    peft_model = get_peft_model(model, lora_config)\n",
        "    peft_model.print_trainable_parameters()  # 查看可训练参数量\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=MODEL_OUTPUT_DIR,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        gradient_accumulation_steps=8,\n",
        "        num_train_epochs=10,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=50,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=200,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=200,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=peft_model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    trainer.train(\n",
        "        # resume_from_checkpoint=True\n",
        "    )\n",
        "    trainer.save_model()\n",
        "    pass"
      ],
      "metadata": {
        "id": "omZpsTgFEQOb"
      },
      "id": "omZpsTgFEQOb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune()"
      ],
      "metadata": {
        "id": "R26dduUrESJT"
      },
      "id": "R26dduUrESJT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}